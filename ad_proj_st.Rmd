---
title: 'PROJECTE ANÀLISI DE DADES: Entrada Turística Espanya'
author: "David Anglada Rotger i Andreu Huguet Segarra"
date: "17/5/2019"
output:
  pdf_document:
    df_print: paged
    number_sections: yes
    toc: yes
  html_document:
    df_print: paged
    toc: yes
fig_width: 5
fig_height: 3
---

# Introducció

En aquest estudi, es realitzarà l'anàlisi de l'entrada turística a Espanya des de l'any 2000 fins a l'actualitat. S'analitzaran les seves tendències generals i estacionals, així com la presència de valors atípics que puguin dificultar-ne l'examinació. L'objectiu de tot plegat és arribar a proposar un model vàlid per poder predir amb la màxima precisió l'entrada turística del proper any.

# Identificació

## Representació gràfica de les dades

Un cop feta la representació de les dades, s'observa una clara tendència creixent. Tot i així, aquesta tendència no és constant, ja que és menys pronunciada entre els anys 2000 i 2010, fins i tot amb una petita baixada entre els anys 2007i 2010 i sembla que es pronuncia a partir de l'any 2011. 

Pel que fa a la variància, s'observa que va augmentant a mesura que augmenta la mitjana dels valors de les dades, és a dir, a mesura que es pronuncia la tendència creixent. És a dir, en els anys 2000-2010, la variància és menor que en els anys 2011-2019, on el creixement augmenta.

```{r echo = FALSE}
serie<-ts(read.table("./data/EntradTur.dat")/1000000,start=2000,freq=12)
plot(serie,main="Entrada Turística a Espanya",ylab="Milions")
abline(v=1999:2019,col=4,lty=3)
```

### Descomposició en components bàsiques

Per poder analitzar millor les dades, es realitza la seva descomposició en les seves components bàsiques, és a dir, el model aditiu de la serie:

$$ X_t = T_t + S_t + C_t + \omega_t $$
on:
- $T_t$ és la **tendència** de la sèrie a llarg termini.
- $S_t$ és el __*seasonal*__ de la sèrie (patró repetit periòdicament amb període constant).
- $C_t$ és el **cicle** de la sèrie (patró repetit periòdicament amb període no constant). Aquesta part no surt representada en la descomposició.
- $\omega_t$ és el soroll aleatòri.

```{r echo = FALSE}
plot(decompose(serie))
```

S'observa, tal i com s'havia comentat anteriorment, la clara tendència creixent de la sèrie, amb un creixement menys pronunciat a l'inici, una petita baixada entre els anys 20017 i 2010 i una pujada més pronunciada més cap a l'actualitat. Pel que fa al patró estacional, observem que durant els mesos d'estiu, el número de turistes a Espanya augmenta molt considerablement. Aquest fet que no crida l'atenció, ja que és durant els mesos d'estiu quan més vacanses s'agafa la gent i més aprofiten per venir a les costes espanyoles. Durant els mesos de tardor-hivern, observem que el número de turistes cau en picat.

## Transformació de les dades

A continuació s'analitzarà la necessitat de realitzar una sèrie de transformacions amb l'objectiu d'aconseguir estacionaritat en la nostra sèrie temporal.

### Variància constant

En primer lloc, s'estudiarà si es pot considerar que la variància de les dades sigui constant en el temps. Ja s'ha comentat que a simple vista semblava que no. Tot i així es comprova amb un plot de la variància front la mitjana i un *boxplot* de les dades cada 12 mesos (que és la freqüència de les nostres dades).

```{r echo = FALSE}
par(mfrow=c(1,2))
m=apply(matrix(serie,nr=12),2,mean)
v=apply(matrix(serie,nr=12),2,var)
plot(m,v,xlab="Mitjanes anuals",ylab="Variàncies anuals",main="Mean-Variance Plot")
abline(lm(v~m),col=2,lty=3,lwd=2)

boxplot(serie~floor(time(serie)))
```

Tal i com s'havia observat a simple vista, la variància augmenta a mesura que agumenta la mitja. Per tant, no podem assumir variància constant. Amb el *boxplot* es confirma aquesta hipòtesis. Així doncs, es procedeix a realitzar una transformació logarítmica de la sèrie per homogeneïtzar la variància. Els resultats obtinguts són els següents:

```{r echo = FALSE, warning=FALSE}
logserie <- log(serie)

par(mfrow=c(1,2))
logm=apply(matrix(logserie,nr=12),2,mean)
logv=apply(matrix(logserie,nr=12),2,var)
plot(logm,logv,xlab="Mitjanes anuals",ylab="Variàncies anuals",main="Mean-Variance Plot")
abline(lm(logv~logm),col=2,lty=3,lwd=2)

boxplot(logserie~floor(time(logserie)))
```

S'observa que la variància s'ha homogeneïtzat, és a dir, ja es pot considerar constant.

### Patró estacional

En segon lloc, s'estudiarà l'existència d'un patró estacional en les nostres dades. En cas que hi sigui present, es realitzarà una diferenciació d'ordre 12, és a dir,

$$ W_t = X_t - X_{t-12} = (1 - B^{12})X_t $$
on $B$ és el *backshift operator*, per eliminar aquest patró. Es realitza un *monthplot* per comprovar-ne l'existència.

```{r echo = FALSE}
monthplot(logserie)
```

Tal i com s'havia comenta, s'observa una clara pujada de la presència de turistes durant els mesos d'estiu i una baixada en picat en l'entrada de l'hivern/tardor. Així doncs, és necessària una diferenciació d'ordre 12 per eliminar aquest patró.

```{r echo = FALSE}
d12logserie <- diff(logserie,12)
monthplot(d12logserie)
```

S'observa que amb una diferenciació d'ordre 12 s'ha eliminat el patró estacional. Ara bé, la mitjana de la sèrie encara no és constant.

### Mitjana constant

Per últim, es vol aconseguir que la sèrie tingui mitjana constant igual (i si és possible igual a 0) per a poder considerar definitivament la sèrie com un procés estacionari. Per aconseguir-ho, es realitzaran diferenciacions regulars de la sèrie fins que s'obtingui el resultat desitjat

$$ W_t = X_t - X_{t-1} = (1 - B)X_t $$

Es realitza la primera diferenciació. Els valors de mitjana i variància aconseguits són els seguents:

```{r echo = FALSE}
d1d12logserie <- diff(d12logserie)
plot(d1d12logserie)
mean(d1d12logserie)
var(d1d12logserie)
```

Com es pot observar, la mitjana del procés diferenciat regularment un cop es pot considerar constant i nula. Ara bé, es mira de diferenciar un segon cop i s'observa que la variància augmenta i, per tant, es té *overdifferentiation*. 

```{r echo = FALSE}
d1d1d12logserie <- diff(d1d12logserie)
plot(d1d1d12logserie)
mean(d1d1d12logserie)
var(d1d1d12logserie)
```

En definitiva, la sèrie transformada pel logaritme, diferenciada un cop i amb una diferenciació d'ordre 12 per eliminar el patró estacional ($\texttt{d1d12logserie}$) és un procés estacionari de mitjana 0.

## ACF/PACF de les dades i proposta de models

Tot seguit, es realitza un anàlisi de les funcions *AutoCorrelació* i de *Correlació Parcial* de la sèrie transformada, és a dir, de la sèrie estacionària.

```{r echo = FALSE}
par(mfrow=c(1,2))
acf(d1d12logserie,ylim=c(-1,1),col=c(2,rep(1,11)),lwd=2,lag.max=72)
pacf(d1d12logserie,ylim=c(-1,1),col=c(rep(1,11),2),lwd=2,lag.max=72)
par(mfrow=c(1,1))
```

### Models proposats per la part regular (p,d,q)

En relació a la part regular de la sèrie, en la funció d'AutoCorrelació (ACF) s'observa un decreixement exponencial alternat en tots els valors. S'observen també valors fora de la banda de confiança en retards llunyans, però poden ser assignats a la aleatorietat del cas 5%. Per tant, en aquest cas, es proposaria $q=0$. En tot cas, si es volgués mirar d'incloure el primer valor que sobresurt més que la resta, es podria considerar també $q=1$.

Pel que fa a la funció de Correlació Parcial (PACF) s'observa que els dos primers valors sobresurten més significativament que la resta. La resta de valors es poden considerar nuls, ja que o bé estan dintre de l'interval de confiança, o bé es poden assignar al cas d'aleatorietat del 5%. Per tant, per la part regular, es proposaria $p=2$.

Donat que s'ha realitzat diferenciació 1 cop, es té que $d=1$. Per tant, els models proposats per la part regular serien $AR(2)$ o, en tot cas, $ARMA(1,1)$ sobre la sèrie transformada regular.

### Models proposats per la part estacional (P,D,Q)

En relació a la part estacional de la sèrie, en la funció d'AutoCorrelació (ACF) s'observa que el primer valor es força significatiu, però també ho són el tercer, el quart i el cinquè, sobretot el quart. Donat que volem intentar proposar un model simplificat, es proposa $Q=0$.

Pel que fa a la funció de Correlació Parcial (PACF) s'observa que sobresurt el primer valor una mica i també sobresurten el tercer i el quart valor. Ara bé, no sobresurten de manera tant significativa com en el cas dels valors del ACF i, per tant, podem assignar-ho al cas d'aleatorietat del 5%. Per tant, en aquest cas, es proposaria $P=1$. En tot cas, es podria proposar $P=4$ per mirar d'incluir aquests valors que sobresurten de la banda de confiança.

Donat que s'ha realitzat una diferenciació d'ordre 12 per eliminar el patró estacional, es té que $D=1$. Per tant, el model proposat per la part regular seria un $AR(1)$

## Models proposats

En conclusió, es proposen per la sèrie diferenciada els models estacionals:

-$ARMA(2,0)(1,0)_s$
-$ARMA(2,0)(4,0)_s$
-$ARMA(1,1)(1,0)_s$

I per la sèrie original, tenint en compte les diferenciacions, es proposen:

-$ARIMA(2,1,0)(1,1,0)_{12}$
-$ARIMA(2,1,0)(4,1,0)_{12}$
-$ARIMA(1,1,1)(1,1,0)_{12}$

# Estimació dels models

A continuació, s'estimen els coeficients dels dos models proposats i es mira que tots siguin significatius. Per mirar-ho, es realitza el test següent (suposant que estem davant d'un model MA:

$$ H_0: \theta_i = 0 $$
$$ H_1: \theta_i \neq 0$$
amb l'estadístic $$ \hat{t} = \frac{\hat{\theta}_i}{\text{se}(\hat{\theta}_i)} $$ ~ $$t-student_{T-k}$$, on k és el nombre total de paràmetres i T és el període. Ara bé, a la pràctica es diu que un coeficient és significant si $|\hat{t}| > 2$.

En primer lloc, s'estimen els coeficients dels models proposats, amb intercept i sense.
```{r, echo = FALSE}
pdq.1 <- c(2, 1, 0)
pdq.2 <- c(1, 1, 1)
PDQ.1 <- c(1, 1, 0)
PDQ.2 <- c(4, 1, 0)

cat("######## ARIMA(2,1,0)(1,1,0) ########\n")
(mod.1.int <- arima(d1d12logserie, order=c(2,0,0), seasonal = list(order = c(1,0,0), period=12)))
(mod.1 <- arima(logserie, order=pdq.1, seasonal = list(order = PDQ.1, period=12)))

cat("\n")
cat("######## ARIMA(2,1,0)(4,1,0) ########\n")
(mod.2.int <- arima(d1d12logserie, order=c(2,0,0), seasonal = list(order = c(4,0,0), period=12)))
(mod.2 <- arima(logserie, order=pdq.1, seasonal = list(order = PDQ.2, period=12)))

cat("\n")
cat("######## ARIMA(1,1,1)(1,1,0) ########\n")
(mod.3.int <- arima(d1d12logserie, order=c(1,0,1), seasonal = list(order = c(1,0,0), period=12)))
(mod.3 <- arima(logserie, order=pdq.2, seasonal = list(order = PDQ.1, period=12)))
```

S'observa que, en cap dels casos, l'intercept no és significatiu i, per tant, es descarten els models amb aquest paràmetre. En el cas del model $ARIMA(2,1,0)(1,1,0)_{12}$, els tres coeficients són significatius. Ara bé, en l'altre model proposat, el model $ARIMA(2,1,0)(4,1,0)_{12}$, s'observa que el coeficient $\texttt{sar3}$ no és significatiu, però la resta de coeficients sí que ho són. Per últim, el model $ARIMA(1,1,1)(1,1,0)_{12}$ té el coeficient $\texttt{ar1}$ no significatiu i els altres dos significatius. 

En termes de *loglikelihood* i de *AIC*, el model que sembla el millor és el model $ARIMA(2,1,0)(4,1,0)_{12}$, que és el millor tant en *AIC* com en *loglikelihood*. Dels altres dos models, donat que el $ARIMA(2,1,0)(1,1,0)_{12}$ té tots els coeficients significatus, es descarta el model $ARIMA(1,1,1)(1,1,0)_{12}$ tot i tenir una mica millor l'*AIC* i la *loglikelihood*. Així doncs, *a-priori*, s'escolliria el primer model $ARIMA(2,1,0)(4,1,0)_{12}$ com a millor model. Tot i així, es realitzarà la validació i la predicció dels dos models escollits en aquest pas.

```{r echo = FALSE}
abs(mod.1.int$coef/sqrt(diag(mod.1.int$var.coef)))>2
abs(mod.1$coef/sqrt(diag(mod.1$var.coef)))>2
abs(mod.2.int$coef/sqrt(diag(mod.2.int$var.coef)))>2
abs(mod.2$coef/sqrt(diag(mod.2$var.coef)))>2
abs(mod.3.int$coef/sqrt(diag(mod.3.int$var.coef)))>2
abs(mod.3$coef/sqrt(diag(mod.3$var.coef)))>2
```

# Validació dels Models

Tot seguit, es realitzarà la validació dels dos models proposat. En el procés de validació es realitzarà un anàlisi dels residus ($Z_t$) dels models, es comprovarà que aquests siguin estacionaris i invertibles, es verificarà la seva estabilitat i s'evaluarà la seva capacitat de previsió.

## Estudi dels residus dels models

Així doncs, en primer lloc, s'estudiaràn els residus del model i es comprovaran els següents aspectes:

- Homogeneïtat de la variància residual ($\sigma_{Z}^2$ constant).
- Normalitat ($Z_T$ ~ Normal).
- Independència ($\rho(k) = 0 \, \, \forall k > 0$).

### Homogeneïtat de la variància

Per comprovar l'homogeneïtat de la variància dels residus, s'analitzen el plot dels mateixos residus, el plot de l'arrel quadrada del seu valor absolut i les funcions ACF i PACF del seu quadrat.

En el cas del primer model ($\texttt{mod.1}$) no s'observa cap tipus de patró (ni creixent ni decreixent) en el plot dels residus o en el plot de l'arrel quadrada del seu valor absolut. A més, en l'ACF i el PACF del quadrat dels residus tots els valors estan dintre de la banda de confiança i, per tant, els podem considerar nuls.

```{r echo = FALSE}
model = mod.1

s=frequency(get(model$series))
resid=model$residuals
par(mfrow=c(1,2),mar=c(3,3,3,3))
#Residuals plot
plot(resid,main="Residuals mod.1")
abline(h=0)
abline(h=c(-3*sd(resid),3*sd(resid)),lty=3,col=4)
#Square Root of absolute values of residuals (Homocedasticity)
scatter.smooth(sqrt(abs(resid)),main="Square Root of Absolute residuals mod.1", lpars=list(col=2))
par(mfrow=c(1,2))
acf(resid^2,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,s-1)),lwd=1, main="Series resid^2 mod.1 ACF")
pacf(resid^2,ylim=c(-1,1),lag.max=60,col=c(rep(1,s-1),2),lwd=1, main="Series resid^2 mod.1 PACF")
par(mfrow=c(1,1))
```

En el cas del segon model ($\texttt{mod.1}$), es poden extreure les mateixes conclusions que en el primer model i, per tant, també es pot assumir homogeneïtat de variància residual.

```{r echo = FALSE}
model = mod.2

s=frequency(get(model$series))
resid=model$residuals
par(mfrow=c(1,2),mar=c(3,3,3,3))
#Residuals plot
plot(resid,main="Residuals mod.2")
abline(h=0)
abline(h=c(-3*sd(resid),3*sd(resid)),lty=3,col=4)
#Square Root of absolute values of residuals (Homocedasticity)
scatter.smooth(sqrt(abs(resid)),main="Square Root of Absolute residuals mod.2", lpars=list(col=2))
par(mfrow=c(1,2))
acf(resid^2,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,s-1)),lwd=1, main="Series resid^2 mod.2 ACF")
pacf(resid^2,ylim=c(-1,1),lag.max=60,col=c(rep(1,s-1),2),lwd=1, main="Series resid^2 mod.2 PACF")
par(mfrow=c(1,1))
```

### Normalitat

Per comprovar la normalitat dels residus dels models proposats s'estudiarà el Q-Q plot, l'histograma dels residus amb la normal que s'hauria de seguir sobreposada i es realitzarà el test de Sharipo-Wilks.

En el cas del model $\texttt{mod.1}$, s'observa en el Q-Q plot que els quartils es situen sobre la línia dels quartils teòrics i que l'histograma s'ajusta a la distribució normal a la que s'hauria d'ajustar (tot i tenir les dues barres més grans una mica per fora de la corba normal). A més, el *p-value* del test de Sharipo-Wilks és $1.713 \times 10^{-05}$, menor que 0.05 i, per tant, es pot assumir la hipòtesi de normalitat en els residus.

```{r echo = FALSE}
model = mod.1

s=frequency(get(model$series))
resid=model$residuals
par(mfrow=c(1,2),mar=c(3,3,3,3))
#Normal plot of residuals
qqnorm(resid)
qqline(resid,col=2,lwd=2)

##Histogram of residuals with normal curve
hist(resid,breaks=20,freq=FALSE)
curve(dnorm(x,mean=mean(resid),sd=sd(resid)),col=2,add=T)

##Shapiro-Wilks Normality test
print(shapiro.test(resid(model)))
```

En el cas del model $\texttt{mod.2}$, les conclusions que s'extreuen són les mateixes. En aquest cas, el *p-value* és de $7.675 \times 10^{-05}$. Per tant, també assumim normalitat en aquest cas. En l'histograma, en aquest cas, només hi ha una barra que sobresurt de la corba normal.

```{r echo = FALSE}
model = mod.2

s=frequency(get(model$series))
resid=model$residuals
par(mfrow=c(1,2),mar=c(3,3,3,3))
#Normal plot of residuals
qqnorm(resid)
qqline(resid,col=2,lwd=2)

##Histogram of residuals with normal curve
hist(resid,breaks=20,freq=FALSE)
curve(dnorm(x,mean=mean(resid),sd=sd(resid)),col=2,add=T)

##Shapiro-Wilks Normality test
print(shapiro.test(resid(model)))
```

### Independència

Per comprovar la independència residual, és a dir, que $\rho(k) = 0$ $\forall k > 0$ s'estudiarà el ACF i el PACF dels residus i es realitzarà el test de Ljung-Box.

Pel que fa al primer model, en primer lloc observem que les funcions ACF i PACF prenen valors pràcticament iguals (menys alguns retards llunyans que en un són positius i en l'altre negatius, però estan dins la banda de confiança en els dos casos), cosa que ja fa intuïr que es complirà la independència. Els residus estandaritzats prenen valors dintre de la franja de (-2,2), la gran majoria, que és el comportament esperat. Ara bé, els *p-values* del test Ljung-Box tenen valors superiors a 0.05 en els primers retards però en els retards llunyans no es pot assumir la independència. Tot i així, donat que en els primers retards sí que es té independència, s'assumeix aquesta hipòtesi pel model.

```{r echo = FALSE}
model = mod.1

s=frequency(get(model$series))
resid=model$residuals

#ACF & PACF of residuals
par(mfrow=c(1,2))
acf(resid,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,s-1)),lwd=1)
pacf(resid,ylim=c(-1,1),lag.max=60,col=c(rep(1,s-1),2),lwd=1)
par(mfrow=c(1,1))

#Ljung-Box p-values
par(mar=c(2,2,1,1))
tsdiag(model,gof.lag=7*s)
```

En el segon model, en canvi, observem més diferències entre les gràfiques del ACF i el PACF. Ara bé, en aquest cas els p-values del test de Ljung-Box estan **tots** per sobre de 0.05 i, per tant, podem assumir independència en tots els residus.

```{r echo = FALSE}
model = mod.2

s=frequency(get(model$series))
resid=model$residuals

#ACF & PACF of residuals
par(mfrow=c(1,2))
acf(resid,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,s-1)),lwd=1)
pacf(resid,ylim=c(-1,1),lag.max=60,col=c(rep(1,s-1),2),lwd=1)
par(mfrow=c(1,1))

#Ljung-Box p-values
par(mar=c(2,2,1,1))
tsdiag(model,gof.lag=7*s)
```

### Estacionaritat i invertibilitat dels models

Per analitzar l'estacionaritat i la invertibilitat dels models proposats, s'expresaran els models com a models $AR(\infty)$ i $MA(\infty)$:

$$ (1 - \phi_1B - \cdots - \phi_pB^p)X_t = (1 + \theta_1B + \cdots + \theta_qB^q)Z_t $$
$$ AR(\infty): \quad \frac{1 - \phi_1B - \cdots - \phi_pB^p}{1 + \theta_1B + \cdots + \theta_qB^q}X_t = (1 - \pi_1B - \pi_2B - \cdots) X_t = Z_t$$
$$ MA(\infty): \quad \frac{1 + \theta_1B + \cdots + \theta_qB^q}{1 - \phi_1B - \cdots - \phi_pB^p}Z_t = (1 + \psi_1B + \psi_2B + \cdots) Z_t = X_t$$
A partir d'aquí el models seran *invertibles* si el mòdul de totes les arrels del polinomi característic $\theta_q(B) = 1 + \theta_1B + \cdots + \theta_qB^q$ és major que 1, és a dir, si $\sum_{i\geq 0} \pi_i^2 < \infty$. Per per altra banda, seran *estacionaris* si el mòdul de totes les arrels del polinomi característic $\phi_q(B) = 1 - \phi_1B - \cdots - \phi_qB^q$ és major que 1, és a dir, si $\sum_{i\geq 0} \psi_i^2 < \infty$.

En el cas del primer model, s'observa que es compleixen totes les condicions i, per tant, el $\texttt{mod.1}$ és estacionari i invertible.

```{r echo = FALSE}
model = mod.1

s=frequency(get(model$series))
resid=model$residuals

#Stationary and Invertible
cat("\nModul of AR Characteristic polynomial Roots: ", 
    Mod(polyroot(c(1,-model$model$phi))),"\n")
cat("\nModul of MA Characteristic polynomial Roots: ",
    Mod(polyroot(c(1,model$model$theta))),"\n")

#Model expressed as an MA infinity (psi-weights)
psis=ARMAtoMA(ar=model$model$phi,ma=model$model$theta,lag.max=36)
names(psis)=paste("psi",1:36)
cat("\nPsi-weights (MA(inf))\n")
cat("\n--------------------\n")
print(psis[1:20])

#Model expressed as an AR infinity (pi-weights)
pis=-ARMAtoMA(ar=-model$model$theta,ma=-model$model$phi,lag.max=36)
names(pis)=paste("pi",1:36)
cat("\nPi-weights (AR(inf))\n")
cat("\n--------------------\n")
print(pis[1:20])
```



```{r echo = FALSE}
model = mod.2

s=frequency(get(model$series))
resid=model$residuals

#Stationary and Invertible
cat("\nModul of AR Characteristic polynomial Roots: ", 
    Mod(polyroot(c(1,-model$model$phi))),"\n")
cat("\nModul of MA Characteristic polynomial Roots: ",
    Mod(polyroot(c(1,model$model$theta))),"\n")

#Model expressed as an MA infinity (psi-weights)
psis=ARMAtoMA(ar=model$model$phi,ma=model$model$theta,lag.max=36)
names(psis)=paste("psi",1:36)
cat("\nPsi-weights (MA(inf))\n")
cat("\n--------------------\n")
print(psis[1:20])

#Model expressed as an AR infinity (pi-weights)
pis=-ARMAtoMA(ar=-model$model$theta,ma=-model$model$phi,lag.max=36)
names(pis)=paste("pi",1:36)
cat("\nPi-weights (AR(inf))\n")
cat("\n--------------------\n")
print(pis[1:20])
```

En el cas del segon model, també es compleix tot i, per tant, també és invertible i estacionari.

### Comparació entre els ACF/PACF mostrals i els ACF/PACF teòrics

Per últim, comparem els valors del ACF i el PACF de les dades amb els valors teòric. S'observa que, en el cas del model $\texttt{mod.2}$, els valors teòrics s'aproximen gairebé perfectament als valors mostrals. En el cas del segon model també es podria dir el mateix. Per tant, ambdós models s'aproximen als valors de ACF/PACF de les mostres, potser una mica millor el $\texttt{mod.2}$ (ja que té més coeficients per calcular els valors teòrics).

```{r echo = FALSE}
model = mod.1
dades = d1d12logserie

s=frequency(get(model$series))
resid=model$residuals


#Sample ACF vs. Teoric ACF
par(mfrow=c(2,3),mar=c(3,3,3,3))
acf(dades, ylim=c(-1,1) ,lag.max=36,main="Sample ACF")

plot(ARMAacf(model$model$phi,model$model$theta,lag.max=36),ylim=c(-1,1), 
     type="h",xlab="Lag",  ylab="", main="ACF Teoric mod.1")
abline(h=0)

model = mod.2

plot(ARMAacf(model$model$phi,model$model$theta,lag.max=36),ylim=c(-1,1), 
     type="h",xlab="Lag",  ylab="", main="ACF Teoric mod.2")
abline(h=0)

model = mod.1

#Sample PACF vs. Teoric PACF
pacf(dades, ylim=c(-1,1) ,lag.max=36,main="Sample PACF")

plot(ARMAacf(model$model$phi,model$model$theta,lag.max=36, pacf=T),ylim=c(-1,1),
     type="h", xlab="Lag", ylab="", main="PACF Teoric mod.1")
abline(h=0)

model = mod.2

plot(ARMAacf(model$model$phi,model$model$theta,lag.max=36, pacf=T),ylim=c(-1,1),
     type="h", xlab="Lag", ylab="", main="PACF Teoric mod.2")
abline(h=0)

par(mfrow=c(1,1))
```

## Estabilitat dels Models

Per comprovar l'estabilitat dels models proposats, calculem els models de la serie ocultant les 12 últimes observacions, és a dir, l'últim període d'observacions. Així doncs, s'observa que el valor dels coeficients varia molt poc, de l'ordre de menys de 0.02 en gairebé tots els casos. Per tant, podem confirmar que els models són estables.

```{r echo = FALSE, warning = FALSE}
ultim=c(2017,12)

serie1=window(serie,end=ultim+c(1,0))
lnserie1=log(serie1)
serie2=window(serie,end=ultim)
lnserie2=log(serie2)

cat("########## Model ARIMA(2,1,0)(1,1,0)12 amb i sense les 12 últimes observacions ##########")
cat("\n")

(mod11 <- arima(lnserie1, order=pdq.1, seasonal=list(order=PDQ.1, period=12)))
(mod12 <- arima(lnserie2, order=pdq.1, seasonal=list(order=PDQ.1, period=12)))

cat("\n")
cat("\n")

cat("########## Model ARIMA(2,1,0)(4,1,0)12 amb i sense les 12 últimes observacions ##########")
cat("\n")

(mod21 <- arima(lnserie1, order=pdq.1, seasonal=list(order=PDQ.2, period=12)))
(mod22 <- arima(lnserie2, order=pdq.1, seasonal=list(order=PDQ.2, period=12)))
```

## Capacitat de predicció

A continuació s'avaluarà la capacitat de predicció dels dos models proposats fent-los predir el valor de les 12 útlimes observacions utilitzant la resta d'observacions conegudes.

```{r echo = FALSE}
#mod.1

pred=predict(mod12,n.ahead=12)
pr<-ts(c(tail(lnserie2,1),pred$pred),start=ultim,freq=12)

se<-ts(c(0,pred$se),start=ultim,freq=12)

#Intervals
tl<-ts(exp(pr-1.96*se),start=ultim,freq=12)
tu<-ts(exp(pr+1.96*se),start=ultim,freq=12)
pr<-ts(exp(pr),start=ultim,freq=12)


ts.plot(serie,tl,tu,pr,
        lty=c(1,2,2,1),col=c(1,4,4,2),xlim=ultim[1]+c(-3,+2),
        type="o",main="Model mod.1 ARIMA(2,1,0)(1,1,0)12")
abline(v=(ultim[1]-3):(ultim[1]+2),lty=3,col=4)

(previs=window(cbind(tl,pr,tu,serie,error=round(serie-pr,3)),start=ultim))

obs=window(serie,start=ultim)
cat("\n")
cat("###### Errors de predicció del model mod.1 ######\n")
(mod.EQM1=sqrt(sum(((obs-pr)/obs)^2)/12))
(mod.EAM1=sum(abs(obs-pr)/obs)/12)


#mod.2

pred=predict(mod22,n.ahead=12)
pr<-ts(c(tail(lnserie2,1),pred$pred),start=ultim,freq=12)

se<-ts(c(0,pred$se),start=ultim,freq=12)

#Intervals
tl<-ts(exp(pr-1.96*se),start=ultim,freq=12)
tu<-ts(exp(pr+1.96*se),start=ultim,freq=12)
pr<-ts(exp(pr),start=ultim,freq=12)


ts.plot(serie,tl,tu,pr,
        lty=c(1,2,2,1),col=c(1,4,4,2),xlim=ultim[1]+c(-3,+2),
        type="o",main="Model mod.2 ARIMA(2,1,1)(4,1,0)12")
abline(v=(ultim[1]-3):(ultim[1]+2),lty=3,col=4)

(previs=window(cbind(tl,pr,tu,serie,error=round(serie-pr,3)),start=ultim))

obs=window(serie,start=ultim)
cat("\n")
cat("###### Errors de predicció del model mod.2 ######\n")
(mod.EQM1=sqrt(sum(((obs-pr)/obs)^2)/12))
(mod.EAM1=sum(abs(obs-pr)/obs)/12)


```

Com es pot veure, les prediccions (en vermell a la gràfica) de les últimes 12 observacions són semblants i prou bones, ja que en amdós casos s'apropen força a la realitat. A més, el valor real de les observacions (en negre a la gràfica) queda dins l'interval de confiança (en blau a la gràfica) dels valors predits. Per tant, es pot concloure que els models tenen bona capacitat de predicció. A més, els errors de predicció (l'**Error Quadràtic Mitjà** i l'**Error Absolut Mitjà**) dels dos models són semblants i molt petits (> 0.05 en ambdós casos).

## Elecció de model

En definitiva, els dos presenten un comportament similar en la predicció de les últimes 12 observacions, però que el test d'independència ha sortit molt millor en el segon model, s'escull el segon model, el model $ARIMA(2,1,1)(4,1,0)_{12}$. A més, com ja s'havia comentat abans, aquest model era el millor en *AIC* i *loglikelihood*.

# Predicció a llarg termini

A continuació, s'utilitza el model escollit $ARIMA(0,1,1)(1,1,0)_{12}$ per predir el valor de la sèrie els 12 mesos posteriors a l'ultima dada que es té. Com es pot observar, el valor predit (en vermell a la gràfica) sembla prou raonable per 2 motius:

- Segueix amb la tendència general de la sèrie de creixement lleu.
- Segueix amb el patró estacional vist al llarg de tota la sèrie: pujada molt pronunciada del número de turistes durant els mesos de primavera i estiu i baixada en picat els mesos de tardor i hivern.

A més, els intervals de confiança també segueixen aquestes tendències estacionals.

```{r echo = FALSE}
##### Previsions a llarg termini amb el model complet ######
mod <- mod.2
pred=predict(mod,n.ahead=12)
pr<-ts(c(tail(logserie,1),pred$pred),start=ultim+c(1,0),freq=12)
se<-ts(c(0,pred$se),start=ultim+c(1,0),freq=12)

#Intervals
tl1<-ts(exp(pr-1.96*se),start=ultim+c(1,0),freq=12)
tu1<-ts(exp(pr+1.96*se),start=ultim+c(1,0),freq=12)
pr1<-ts(exp(pr),start=ultim+c(1,0),freq=12)

ts.plot(serie,tl1,tu1,pr1,
        lty=c(1,2,2,1), col=c(1,4,4,2), xlim=c(ultim[1]-2,ultim[1]+3),
        type="o",main="Model ARIMA(0,1,1)(1,1,0)12")
abline(v=(ultim[1]-2):(ultim[1]+3),lty=3,col=4)
```

```{r echo = FALSE}
(previs1=window(cbind(tl1,pr1,tu1),start=ultim+c(1,0)))
```

# Tractament de *outliers*

Per acabar l'anàlisi d'aquesta serie, es centrarà l'atenció en la detecció i la correcció de possibles *outliers* en la serie. Aquests valors atípics poden ser de tres tipus diferents: 

- **Outlier Aditiu (AO)**: Afecta només a un període ($X_t = 1_{t=TO}(t)$).
- **Canvi Transitori (TC)**: Afecta a un període i el seu efecte decreix exponencialment en els següents períodes ($X_t = \delta^{(t-T0)}1_{t \geq TO}(t)$).
- **Level Shift (LS)**: Afecta a un període i el seu efecte es manté durant els següents períodes ($X_t = 1_{t \geq TO}(t)$).

A continuació es mostren els *outliers* detectats, així com la seva influència sobre les dades i la data en què tenen aquest efecte. S'observa que tenim 10 valors atípics en total, dels quals la majoria són puntuals (6 són AO), 2 són TC i tenim 2 LS. L'*outlier* que més efecte ha tingut és del març del 2002, que ha tingut a més un efecte positiu, és a dir, ha fet créixer la sèrie, tot i només afectar a aquell període, ja que era era de tipus AO. En segon lloc, tenim un del tipus LS a l'abril del 2008. Aquest últim es podria associar perfectament a l'entrada en crisi econòmica del pais.

```{r echo = FALSE}
source("./airbcn/atipics2.R")

# Detecció outliers
mod.outliers=outdetec(mod,dif=c(1,12),crit=2.8,LS=T)

# Out
outliers=mod.outliers$atip[order(mod.outliers$atip[,1]),]
mesos=c("Gen","Feb","Març","Abr","Maig","Juny","Jul","Ago","Sep","Oct","Nov","Dic")
data.frame(outliers,Fecha=paste(mesos[(outliers[,1]-1)%%12+1],start(logserie)[1]+((outliers[,1]-1)%/%12)))
mod.outliers$sigma2

data.frame(outliers,Fecha=paste(mesos[(outliers[,1]-1)%%12+1],start(logserie)[1]+((outliers[,1]-1)%/%12)),perc.Obs=exp(outliers[,3])*100)
```

En el següent gràfic, es poden observar la gràfica amb els *outliers* (en negre) i la gràfica linealitzada, és a dir, sense *outliers* (en vermell). S'observa com hi ha outliers que han provocat que la sèrie tingui valors més baixos i outliers que han provocat que la sèrie tingui valors més alts. El fet que crida més l'atenció és que, a causa dels outliers, la sèrie des del 2005 (sobretot a partir del 2008) té valors més baixos del que hauria de tenir (s'observen els pics negres per sota dels vermells).

```{r echo = FALSE}

##Comparaci?n serie observada con la serie linealizada (sin at?picos)
logserie.lin=lineal(logserie,mod.outliers$atip)
serie.lin=exp(logserie.lin)

plot(serie.lin,col=2)
lines(serie)

plot(logserie.lin, col=2)
lines(logserie)
```

Per veure més clar l'efecte dels outliers, s'exposa la gràfica dels valors de la sèrie menys els valos de la sèrie linealitzada. Com es pot veure, l'abril del 2008 hi ha un *outlier* de tipus LS que fa que la sèrie agafi valors més petits des d'aquesta data en endavant. Crida l'atenció també els pics amunt i avall del principi de la sèrie, a les dates de març del 2002 i abril de 2002, que en només un més de diferència es va tenir una gran entrada turística el març i una baixa entrada turísica l'abril. Per últim, cal remarcar un *outlier* de tipus LS a l'octubre de 2001, que també fa que des de llavors la sèrie prengui valors inferiors.

```{r echo = FALSE}
##Efecto de los at?picos en la serie de logaritmos
plot(logserie-logserie.lin)
```

## Identificació i estimació del model per la sèrie linealitzada

Un cop eliminats els *outliers* de la sèrie, es calculen un altre cop les funcions ACF i PACF de la sèrie linealitzada. S'observa clarament que, pel que fa a la part regular, al ACF es té decreixement exponencial alternat durant tots els valors, tinguent valors infinits fora de la banda de confiança als valors inicials. Pel que fa al PACF, observem que els dos primers valos són encara més significants que en el cas del PACF de la sèrie sense linealitzar. També hi ha altres valors fora de la banda que poden associar-se al cas del 5%. Per tant, pel que fa a la part regular, igual que en el cas de la sèrie amb valors atípics, 
es confirma la hipòtesi que el model adequat és un AR(2).

Pel que fa a la part estacional, l'anàlisi és molt semblant al de la sèrie sense linealitzar: un ACF amb força valors fora de la banda de confiança, sobretot el quart valor (molt més que el primer) i un PACF on es podrien considerar significatius el primer, el tercer i el quart valor. Per tant, igual que en el cas de la sèrie sense linealitzar, proposem un AR(4) per la part estacional.

```{r echo = FALSE}
##Identificació del model per la sèrie linealitzada

d1d12logserie.lin=diff(diff(logserie.lin,12))
par(mfrow=c(1,2))
acf(d1d12logserie.lin,ylim=c(-1,1),lag.max=72,col=c(2,rep(1,11)),lwd=2)
pacf(d1d12logserie.lin,ylim=c(-1,1),lag.max=72,col=c(rep(1,11),2),lwd=2)
par(mfrow=c(1,1))
```

Ara bé, a diferència del que s'ha vist anteriorment, en aquest cas surten no significatius tant el coeficient de $\texttt{sar2}$ com el de $\texttt{sar3}$

```{r echo = FALSE}
##Estimació del model per la sèrie linealitzada

(mod.lin=arima(logserie.lin,order=pdq.1,seasonal=list(order=PDQ.2,period=12)))
abs(mod.lin$coef/sqrt(diag(mod.lin$var.coef)))>2
```

## Validació del model per la sèrie linealitzada

Pel que fa a la validació del model per la sèrie linealitzada, s'observa en les gràfiques els mateixos anàlisis realitzats anteriorment, és a dir:

- Es conclueix que es poden assumir les hipòtesis d'homogeneïtat en la variància dels residus (no patrons en les gràfiques de la seva variància i ACF i PACF dels residus al quadrat nuls), de normalitat dels residus (Q-Q plot amb relació lineal, histograma s'ajusta a la corba normal) i d'independència dels residus (*p-values* de Ljung-Box per sobre de 0.05 i ACF i PACF dels residus molt iguals).

- Es pot dir que el model és causal i invertible, ja que totes les arrels dels polinomis característics tenen mòdul major que un.

- El ACF i el PACF teòrics són molt semblants al ACF i PACF mostrals.

Per tant, es conclou que el model per la sèrie linealitzada és un model vàlid.


```{r echo = FALSE}
dades=d1d12logserie.lin
model=mod.lin
source("validation.r")
validation(model,dades)
```

## Estabilitat del model proposat per la sèrie linealitzada

En relació a l'estabilitat del model per la sèrie linealitzada, s'observa que el valor dels coeficients varia molt poc, de l'ordre de menys de 0.02 en gairebé tots els casos. Per tant, podem confirmar que és estable.

```{r echo = FALSE, warning=FALSE}
ultim=c(2017,12)

serie1.lin=window(serie.lin,end=ultim+c(1,0))
logserie1.lin=log(serie1.lin)
serie2.lin=window(serie.lin,end=ultim)
logserie2.lin=log(serie2.lin)

(mod.lin=arima(logserie1.lin,order=pdq.1,seasonal=list(order=PDQ.2,period=12)))
(mod2.lin=arima(logserie2.lin,order=pdq.1,seasonal=list(order=PDQ.2,period=12)))
```

## Capacitat de predicció del model proposat per la sèrie linealitzada

Pel que fa a la capacitat de predicció del model per la sèrie linealitzada, es pot observar que és millor que el model per la sèrie sense linealitzar, ja que s'ajusta molt més (de fet, tenim tant el EQM com el EAM més baix en aquest cas). De fet, en les zones de pujada i baixada els intervals de confiança estan gairebé a sobre del valor real de la sèrie. En els valors més alts és on es té més error, fet que no extranya, ja que és en els mesos de l'any on més ha anat variant el valor de la sèrie al llarg dels anys

```{r echo = FALSE}
pred=predict(mod2.lin,n.ahead=12)
wLS=sum(mod.outliers$atip[mod.outliers$atip$type_detected=="LS" & mod.outliers$atip$Obs<=length(serie)-12,3])
predic=pred$pr+wLS
pr<-ts(c(tail(lnserie2,1),predic),start=ultim,freq=12)
se<-ts(c(0,pred$se),start=ultim,freq=12)

#Intervals
tl<-ts(exp(pr-1.96*se),start=ultim,freq=12)
tu<-ts(exp(pr+1.96*se),start=ultim,freq=12)
pr<-ts(exp(pr),start=ultim,freq=12)

ts.plot(serie,tl,tu,pr,lty=c(1,2,2,1),col=c(1,4,4,2),xlim=ultim[1]+c(-3,+2),type="o",main="Model ARIMA(2,1,0)(4,1,0)12")
abline(v=(ultim[1]-3):(ultim[1]+2),lty=3,col=4)
```

```{r echo = FALSE}
(previs.lin=window(cbind(tl,pr,tu,serie,error=round(serie-pr,3)),start=ultim))
```

```{r echo = FALSE}
obs=window(serie,start=ultim)
(mod.EQM2=sqrt(sum(((obs-pr)/obs)^2)/12))
(mod.EAM2=sum(abs(obs-pr)/obs)/12)
```

## Previsions a llarg termini pel model proposat per la sèrie linealitzada

```{r echo = FALSE}
##### Previsions a llarg termini amb el model complet ######

pred=predict(mod.lin,n.ahead=12)
wLS=sum(mod.outliers$atip[mod.outliers$atip$type_detected=="LS",3])
predic=pred$pr+wLS
pr<-ts(c(logserie[length(logserie)],predic),start=ultim+c(1,0),freq=12)
se<-ts(c(0,pred$se),start=ultim+c(1,0),freq=12)

#Intervals
tl2<-ts(exp(pr-1.96*se),start=ultim+c(1,0),freq=12)
tu2<-ts(exp(pr+1.96*se),start=ultim+c(1,0),freq=12)
pr2<-ts(exp(pr),start=ultim+c(1,0),freq=12)

ts.plot(serie,tl2,tu2,pr2,lty=c(1,2,2,1),col=c(1,4,4,2),xlim=ultim[1]+c(-1,+3),type="o",main="Model ARIMA(2,1,0)(4,1,0)12")
abline(v=(ultim[1]-2):(ultim[1]+3),lty=3,col=4)
```

# Comparació dels dos models

Com a última part d'aquest anàlisi, es realitza una comparació entre els valors predits pel model per la sèrie sense linealitzar i els valors predits per la sèrie linealitzada (és a dir, sense *outliers*). Tot i que els valors són força semblants, el que crida més l'atenció és que l'interval de confiança d'aquests valors és més estret en el cas del model per la sèrie linealitzada. Aquest fet es deu a que la variància dels valors sense *outliers* és molt més petita i, per tant, hi ha més marge d'acotació a l'hora de calcular tant els valors com els intervals. Per tant, podem dir que el model sense *outliers* és millor

```{r echo = FALSE}
(previs2=window(cbind(tl2,pr2,tu2),start=ultim+c(1,0)))

cbind(previs1,previs2)
```

```{r echo = FALSE}
ts.plot(serie,tl1,tu1,pr1,tl2,tu2,pr2,lty=c(1,2,2,1,2,2,1),col=c(1,4,4,2,3,3,6),xlim=ultim[1]+c(1,3),type="o",main="Entrada Turística a Espanya")
legend("topleft",c("ARIMA(2,1,0)(4,1,0)12","ARIMA(2,1,0)(4,1,0)12 with outlier treatment"),col=c(4,3),lty=1,lwd=2)
abline(v=ultim[1]+1:3,lty=3,col=4)
```

Per acabar de confirmar-ho, es mostren tota una sèrie de mesures de bondat d'ajust dels models. Sobretot ens crida l'atenció el *AIC* i el *BIC*, on el model 
sense *outliers* és clarament millor. Aquest model també té millor RMSPE i MAPE.

```{r echo = FALSE}
resul=data.frame(
  par=c(length(coef(mod)),length(coef(mod.lin))+nrow(mod.outliers$atip)),
  Sigma2Z=c(mod$sigma2,mod.lin$sigma2),
  AIC=c(AIC(mod),AIC(mod.lin)+2*nrow(mod.outliers$atip)),
  BIC=c(BIC(mod),BIC(mod.lin)+log(length(serie)-13)*nrow(mod.outliers$atip)),
  RMSPE=c(mod.EQM1,mod.EQM2),
  MAPE=c(mod.EAM1,mod.EAM2),
  meanLength=c(sum(previs1[,3]-previs1[,1]),sum(previs2[,3]-previs2[,1]))/12)
row.names(resul)=c("ARIMA(2,1,0)(4,1,0)12","ARIMA(2,1,0)(4,1,0)12+Outliers")

resul
head(resul)
```

# Comentaris finals

Així doncs, un com realitzat aquest anàlisi, es conclou que la presència de valors atípics en una sèrie pot influir (i molt) en les previsions que pugui fer un model basant-se en ella. Per tant, es confirma la importància de la seva detecció i correcció. Pel que fa a les previsions de la sèrie, sembla que la tendència general de creixement es mantindrà, així com també el patró estacional (és a dir, que Espanya tenint un munt de turistes a l'estiu).

(En l'apartat de models proposats, s'ha volgut proposar també un $ARIMA(1,1,1)(4,1,0)_{12}$, però per problemes del *R* no s'ha pogut calcular.)