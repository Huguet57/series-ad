---
title: 'PROJECTE ANÀLISI DE DADES: Entrada Turística Espanya'
author: "David Anglada Rotger i Andreu Huguet Segarra"
date: "17/5/2019"
output:
  pdf_document:
    df_print: paged
    number_sections: yes
    toc: yes
  html_document:
    df_print: paged
    toc: yes
fig_width: 5
fig_height: 3
---

# Introducció

# Identificació

## Representació gràfica de les dades

Un cop feta la representació de les dades, s'observa una clara tendència creixent. Tot i així, aquesta tendència no és constant, ja que és menys pronunciada entre els anys 2000 i 2010, fins i tot amb una petita baixada entre els anys 2007i 2010 i sembla que es pronuncia a partir de l'any 2011. 

Pel que fa a la variància, s'observa que va augmentant a mesura que augmenta la mitjana dels valors de les dades, és a dir, a mesura que es pronuncia la tendència creixent. És a dir, en els anys 2000-2010, la variància és menor que en els anys 2011-2019, on el creixement augmenta.

```{r echo = FALSE}
serie<-ts(read.table("./data/EntradTur.dat")/1000000,start=2000,freq=12)
plot(serie,main="Entrada Turística a Espanya",ylab="Milions")
abline(v=1999:2019,col=4,lty=3)
```

### Descomposició en components bàsiques

Per poder analitzar millor les dades, es realitza la seva descomposició en les seves components bàsiques, és a dir, el model aditiu de la serie:

$$ X_t = T_t + S_t + C_t + \omega_t $$
on:
- $T_t$ és la **tendència** de la sèrie a llarg termini.
- $S_t$ és el __*seasonal*__ de la sèrie (patró repetit periòdicament amb període constant).
- $C_t$ és el **cicle** de la sèrie (patró repetit periòdicament amb període no constant). Aquesta part no surt representada en la descomposició.
- $\omega_t$ és el soroll aleatòri.

```{r echo = FALSE}
plot(decompose(serie))
```

S'observa, tal i com s'havia comentat anteriorment, la clara tendència creixent de la sèrie, amb un creixement menys pronunciat a l'inici, una petita baixada entre els anys 20017 i 2010 i una pujada més pronunciada més cap a l'actualitat. Pel que fa al patró estacional, observem que durant els mesos d'estiu, el número de turistes a Espanya augmenta molt considerablement. Aquest fet que no crida l'atenció, ja que és durant els mesos d'estiu quan més vacanses s'agafa la gent i més aprofiten per venir a les costes espanyoles. Durant els mesos de tardor-hivern, observem que el número de turistes cau en picat.

## Transformació de les dades

A continuació s'analitzarà la necessitat de realitzar una sèrie de transformacions amb l'objectiu d'aconseguir estacionaritat en la nostra sèrie temporal.

### Variància constant

En primer lloc, s'estudiarà si es pot considerar que la variància de les dades sigui constant en el temps. Ja s'ha comentat que a simple vista semblava que no. Tot i així es comprova amb un plot de la variància front la mitjana i un *boxplot* de les dades cada 12 mesos (que és la freqüència de les nostres dades).

```{r echo = FALSE}
par(mfrow=c(1,2))
m=apply(matrix(serie,nr=12),2,mean)
v=apply(matrix(serie,nr=12),2,var)
plot(m,v,xlab="Mitjanes anuals",ylab="Variàncies anuals",main="Mean-Variance Plot")
abline(lm(v~m),col=2,lty=3,lwd=2)

boxplot(serie~floor(time(serie)))
```

Tal i com s'havia observat a simple vista, la variància augmenta a mesura que agumenta la mitja. Per tant, no podem assumir variància constant. Amb el *boxplot* es confirma aquesta hipòtesis. Així doncs, es procedeix a realitzar una transformació logarítmica de la sèrie per homogeneïtzar la variància. Els resultats obtinguts són els següents:

```{r echo = FALSE, warning=FALSE}
logserie <- log(serie)

par(mfrow=c(1,2))
logm=apply(matrix(logserie,nr=12),2,mean)
logv=apply(matrix(logserie,nr=12),2,var)
plot(logm,logv,xlab="Mitjanes anuals",ylab="Variàncies anuals",main="Mean-Variance Plot")
abline(lm(logv~logm),col=2,lty=3,lwd=2)

boxplot(logserie~floor(time(logserie)))
```

S'observa que la variància s'ha homogeneïtzat, és a dir, ja es pot considerar constant.

### Patró estacional

En segon lloc, s'estudiarà l'existència d'un patró estacional en les nostres dades. En cas que hi sigui present, es realitzarà una diferenciació d'ordre 12, és a dir,

$$ W_t = X_t - X_{t-12} = (1 - B^{12})X_t $$
on $B$ és el *backshift operator*, per eliminar aquest patró. Es realitza un *monthplot* per comprovar-ne l'existència.

```{r echo = FALSE}
monthplot(logserie)
```

Tal i com s'havia comenta, s'observa una clara pujada de la presència de turistes durant els mesos d'estiu i una baixada en picat en l'entrada de l'hivern/tardor. Així doncs, és necessària una diferenciació d'ordre 12 per eliminar aquest patró.

```{r echo = FALSE}
d12logserie <- diff(logserie,12)
monthplot(d12logserie)
```

S'observa que amb una diferenciació d'ordre 12 s'ha eliminat el patró estacional. Ara bé, la mitjana de la sèrie encara no és constant.

### Mitjana constant

Per últim, es vol aconseguir que la sèrie tingui mitjana constant igual (i si és possible igual a 0) per a poder considerar definitivament la sèrie com un procés estacionari. Per aconseguir-ho, es realitzaran diferenciacions regulars de la sèrie fins que s'obtingui el resultat desitjat

$$ W_t = X_t - X_{t-1} = (1 - B)X_t $$

Es realitza la primera diferenciació. Els valors de mitjana i variància aconseguits són els seguents:

```{r echo = FALSE}
d1d12logserie <- diff(d12logserie)
plot(d1d12logserie)
mean(d1d12logserie)
var(d1d12logserie)
```

Com es pot observar, la mitjana del procés diferenciat regularment un cop es pot considerar constant i nula. Ara bé, es mira de diferenciar un segon cop i s'observa que la variància augmenta i, per tant, es té *overdifferentiation*. 

```{r echo = FALSE}
d1d1d12logserie <- diff(d1d12logserie)
plot(d1d1d12logserie)
mean(d1d1d12logserie)
var(d1d1d12logserie)
```

En definitiva, la sèrie transformada pel logaritme, diferenciada un cop i amb una diferenciació d'ordre 12 per eliminar el patró estacional ($\texttt{d1d12logserie}$) és un procés estacionari de mitjana 0.

## ACF/PACF de les dades i proposta de models

Tot seguit, es realitza un anàlisi de les funcions *AutoCorrelació* i de *Correlació Parcial* de la sèrie transformada, és a dir, de la sèrie estacionària.

```{r echo = FALSE}
par(mfrow=c(1,2))
acf(d1d12logserie,ylim=c(-1,1),col=c(2,rep(1,11)),lwd=2,lag.max=72)
pacf(d1d12logserie,ylim=c(-1,1),col=c(rep(1,11),2),lwd=2,lag.max=72)
par(mfrow=c(1,1))
```

### Models proposats per la part regular (p,d,q)

En relació a la part regular de la sèrie, en la funció d'AutoCorrelació (ACF) s'observa que només sobresurt el primer valor. La resta de valors es poden considerar nuls, ja que o bé estan dintre de l'interval de confiança, o bé es poden assignar al cas d'aleatorietat del 5%. Per tant, per la part regular, es proposaria $q=1$.

Pel que fa a la funció de Correlació Parcial (PACF) s'observa un decreixement exponencial dels primers valors. S'observen també valors fora de la banda de confiança, però poden ser assignats a la aleatorietat del cas 5%. Per tant, en aquest cas, es proposaria $p=0$. En tot cas, si es volgués mirar d'incloure el primer valor que sobresurt més que la resta, es podria considerar també $p=1$.

Donat que s'ha realitzat diferenciació 1 cop, es té que $d=1$. Per tant, els models proposats per la part regular serien $MA(1)$ o, en tot cas, $ARMA(1,1)$ sobre la sèrie transformada regular.

## Models proposats per la part estacional (P,D,Q)

En relació a la part estacional de la sèrie, en la funció d'AutoCorrelació (ACF) s'observa que el primer valor es força significatiu, però també ho són el tercer, el quart i el cinquè, sobretot el quart. Donat que volem intentar proposar un model simplificat, es proposa $Q=0$.

Pel que fa a la funció de Correlació Parcial (PACF) s'observa que sobresurt el primer valor una mica i també sobresurten el tercer i el quart valor. Ara bé, no sobresurten de manera tant significativa com en el cas dels valors del ACF i, per tant, podem assignar-ho al cas d'aleatorietat del 5%. Per tant, en aquest cas, es proposaria $P=1$.

Donat que s'ha realitzat una diferenciació d'ordre 12 per eliminar el patró estacional, es té que $D=1$. Per tant, el model proposat per la part regular seria un $AR(1)$

## Models proposats

En conclusió, es proposen per la sèrie diferenciada els models estacionals:

-$ARMA(0,1)(1,0)_s$
-$ARMA(1,1)(1,0)_s$

I per la sèrie original, tenint en compte les diferenciacions, es proposen:

-$SARIMA(0,1,1)(1,1,0)_s$
-$SARIMA(1,1,1)(1,1,0)_s$

# Estimació dels models

A continuació, s'estimen els coeficients dels dos models proposats i es mira que tots siguin significatius. Per mirar-ho, es realitza el test següent (suposant que estem davant d'un model MA:

$$ H_0: \theta_i = 0 $$
$$ H_1: \theta_i \neq 0$$
amb l'estadístic $$ \hat{t} = \frac{\hat{\theta}_i}{\text{se}(\hat{\theta}_i)} $$ ~ $$t-student_{T-k}$$, on k és el nombre total de paràmetres i T és el període. Ara bé, a la pràctica es diu que un coeficient és significant si $|\hat{t}| > 2$.

En primer lloc, s'estimen els coeficients dels models proposats, amb intercept i sense.
```{r, echo = FALSE}
pdq.1 <- c(0, 1, 1)
PDQ.1 <- c(1, 1, 0)
(mod.1.int <- arima(d1d12logserie, order=c(0,0,1), seasonal = list(order = c(1,0,0), period=12)))
(mod.1 <- arima(logserie, order=pdq.1, seasonal = list(order = PDQ.1, period=12)))
pdq.2 <- c(1, 1, 1)
PDQ.2 <- c(1, 1, 0)
(mod.2.int <- arima(d1d12logserie, order=c(1,0,1), seasonal = list(order = c(1,0,0), period=12)))
(mod.2 <- arima(logserie, order=pdq.2, seasonal = list(order = PDQ.2, period=12)))
```

S'observa que, en ambdós casos, l'intercept no és significatiu i, per tant, es descarten aquests dos models. En el cas del model $SARIMA(0,1,1)(1,1,0)_s$, els dos coeficients són significatius. Ara bé, en l'altre model proposat, el model $SARIMA(1,1,1)(1,1,0)_s$, s'observa que el coeficient $\texttt{ar1}$ no és significatiu. Tot i així, el primer model proposat ($SARIMA(0,1,1)(1,1,0)_s$), té un pitjor AIC que l'altre model i té una *loglikelihood* més baixa. Ara bé, donat que eliminant el coeficient no significatiu del segon model, queda el primer model, *a-priori* s'escolliria el primer model. Falta, òbviament, la seva validació.

```{r echo = FALSE}
abs(mod.1.int$coef/sqrt(diag(mod.1.int$var.coef)))>2
abs(mod.1$coef/sqrt(diag(mod.1$var.coef)))>2
abs(mod.2.int$coef/sqrt(diag(mod.2.int$var.coef)))>2
abs(mod.2$coef/sqrt(diag(mod.2$var.coef)))>2
```

# Validació dels Models

Tot seguit, es realitzarà la validació dels dos models proposat. En el procés de validació es realitzarà un anàlisi dels residus ($Z_t$) dels models, es comprovarà que aquests siguin estacionaris i invertibles, es verificarà la seva estabilitat i s'evaluarà la seva capacitat de previsió.

## Estudi dels residus dels models

Així doncs, en primer lloc, s'estudiaràn els residus del model i es comprovaran els següents aspectes:

- Homogeneïtat de la variància residual ($\sigma_{Z}^2$ constant).
- Normalitat ($Z_T$ ~ Normal).
- Independència ($\rho(k) = 0 \, \, \forall k > 0$).

### Homogeneïtat de la variància

Per comprovar l'homogeneïtat de la variància dels residus, s'analitzen el plot dels mateixos residus, el plot de l'arrel quadrada del seu valor absolut i les funcions ACF i PACF del seu quadrat.

En el cas del primer model ($\texttt{mod.1}$) no s'observa cap tipus de patró (ni creixent ni decreixent) en el plot dels residus o en el plot de l'arrel quadrada del seu valor absolut. A més, en l'ACF i el PACF del quadrat dels residus tots els valors estan dintre de la banda de confiança i, per tant, els podem considerar nuls.

```{r echo = FALSE}
model = mod.1

s=frequency(get(model$series))
resid=model$residuals
par(mfrow=c(1,2),mar=c(3,3,3,3))
#Residuals plot
plot(resid,main="Residuals mod.1")
abline(h=0)
abline(h=c(-3*sd(resid),3*sd(resid)),lty=3,col=4)
#Square Root of absolute values of residuals (Homocedasticity)
scatter.smooth(sqrt(abs(resid)),main="Square Root of Absolute residuals mod.1", lpars=list(col=2))
par(mfrow=c(1,2))
acf(resid^2,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,s-1)),lwd=1, main="Series resid^2 mod.1 ACF")
pacf(resid^2,ylim=c(-1,1),lag.max=60,col=c(rep(1,s-1),2),lwd=1, main="Series resid^2 mod.1 PACF")
par(mfrow=c(1,1))
```

En el cas del segon model ($\texttt{mod.1}$), es poden extreure les mateixes conclusions que en el primer model i, per tant, també es pot assumir homogeneïtat de variància residual.

```{r echo = FALSE}
model = mod.2

s=frequency(get(model$series))
resid=model$residuals
par(mfrow=c(1,2),mar=c(3,3,3,3))
#Residuals plot
plot(resid,main="Residuals mod.2")
abline(h=0)
abline(h=c(-3*sd(resid),3*sd(resid)),lty=3,col=4)
#Square Root of absolute values of residuals (Homocedasticity)
scatter.smooth(sqrt(abs(resid)),main="Square Root of Absolute residuals mod.2", lpars=list(col=2))
par(mfrow=c(1,2))
acf(resid^2,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,s-1)),lwd=1, main="Series resid^2 mod.2 ACF")
pacf(resid^2,ylim=c(-1,1),lag.max=60,col=c(rep(1,s-1),2),lwd=1, main="Series resid^2 mod.2 PACF")
par(mfrow=c(1,1))
```

### Normalitat

Per comprovar la normalitat dels residus dels models proposats s'estudiarà el Q-Q plot, l'histograma dels residus amb la normal que s'hauria de seguir sobreposada i es realitzarà el test de Sharipo-Wilks.

En el cas del model $\texttt{mod.1}$, s'observa en el Q-Q plot que els quartils es situen sobre la línia dels quartils teòrics i que l'histograma s'ajusta a la distribució normal a la que s'hauria d'ajustar. A més, el *p-value* del test de Sharipo-Wilks és $4.879 \times 10^{-05}$, menor que 0.05 i, per tant, es pot assumir la hipòtesi de normalitat en els residus.

```{r echo = FALSE}
model = mod.1

s=frequency(get(model$series))
resid=model$residuals
par(mfrow=c(1,2),mar=c(3,3,3,3))
#Normal plot of residuals
qqnorm(resid)
qqline(resid,col=2,lwd=2)

##Histogram of residuals with normal curve
hist(resid,breaks=20,freq=FALSE)
curve(dnorm(x,mean=mean(resid),sd=sd(resid)),col=2,add=T)

##Shapiro-Wilks Normality test
print(shapiro.test(resid(model)))
```

En el cas del model $\texttt{mod.2}$, les conclusions que s'extreuen són les mateixes. En aquest cas, el *p-value* és de $7.675 \times 10^{-05}$. Per tant, també assumim normalitat en aquest cas.

```{r echo = FALSE}
model = mod.2

s=frequency(get(model$series))
resid=model$residuals
par(mfrow=c(1,2),mar=c(3,3,3,3))
#Normal plot of residuals
qqnorm(resid)
qqline(resid,col=2,lwd=2)

##Histogram of residuals with normal curve
hist(resid,breaks=20,freq=FALSE)
curve(dnorm(x,mean=mean(resid),sd=sd(resid)),col=2,add=T)

##Shapiro-Wilks Normality test
print(shapiro.test(resid(model)))
```

### Independència

Per comprovar la independència residual, és a dir, que $\rho(k) = 0$ $\forall k > 0$ s'estudiarà el ACF i el PACF dels residus i es realitzarà el test de Ljung-Box.

Pel que fa al primer model, en primer lloc observem que les funcions ACF i PACF prenen valors pràcticament iguals, cosa que ja fa intuïr que es complirà la independència. Els residus estandaritzats prenen valors dintre de la franja de (-2,2), la gran majoria, que és el comportament esperat. A més, els *p-values* del test Ljung-Box, que gairebé tots menors que 0.05, confirmen que es pot assumir la independència dels residus.

```{r echo = FALSE}
model = mod.1

s=frequency(get(model$series))
resid=model$residuals

#ACF & PACF of residuals
par(mfrow=c(1,2))
acf(resid,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,s-1)),lwd=1)
pacf(resid,ylim=c(-1,1),lag.max=60,col=c(rep(1,s-1),2),lwd=1)
par(mfrow=c(1,1))

#Ljung-Box p-values
par(mar=c(2,2,1,1))
tsdiag(model,gof.lag=7*s)
```

En el segon model, l'anàlisi es pràcticament el mateix, tret que, en aquest cas, els *p-values* els costa més assolir un valor per sota de 0.05. Tot i així, també podem assumir la independència (tot i que no de manera tant clara com en el cas anterior).

```{r echo = FALSE}
model = mod.2

s=frequency(get(model$series))
resid=model$residuals

#ACF & PACF of residuals
par(mfrow=c(1,2))
acf(resid,ylim=c(-1,1),lag.max=60,col=c(2,rep(1,s-1)),lwd=1)
pacf(resid,ylim=c(-1,1),lag.max=60,col=c(rep(1,s-1),2),lwd=1)
par(mfrow=c(1,1))

#Ljung-Box p-values
par(mar=c(2,2,1,1))
tsdiag(model,gof.lag=7*s)
```

### Estacionaritat i invertibilitat dels models

Per analitzar l'estacionaritat i la invertibilitat dels models proposats, s'expresaran els models com a models $AR(\infty)$ i $MA(\infty)$:

$$ (1 - \phi_1B - \cdots - \phi_pB^p)X_t = (1 + \theta_1B + \cdots + \theta_qB^q)Z_t $$
$$ AR(\infty): \quad \frac{1 - \phi_1B - \cdots - \phi_pB^p}{1 + \theta_1B + \cdots + \theta_qB^q}X_t = (1 - \pi_1B - \pi_2B - \cdots) X_t = Z_t$$
$$ MA(\infty): \quad \frac{1 + \theta_1B + \cdots + \theta_qB^q}{1 - \phi_1B - \cdots - \phi_pB^p}Z_t = (1 + \psi_1B + \psi_2B + \cdots) Z_t = X_t$$
A partir d'aquí el models seran *invertibles* si el mòdul de totes les arrels de $\theta_q(B) = 1 + \theta_1B + \cdots + \theta_qB^q$ és major que 1, és a dir, si $\sum_{i\geq 0} \pi_i^2 < \infty$. Per per altra banda, seran *estacionaris* si el mòdul de totes les arrels de $\phi_q(B) = 1 - \phi_1B - \cdots - \phi_qB^q$ és major que 1, és a dir, si $\sum_{i\geq 0} \psi_i^2 < \infty$.

En el cas del primer model, s'observa que es compleixen totes les condicions i, per tant, el $\texttt{mod.1}$ és estacionari i invertible.

```{r echo = FALSE}
model = mod.1

s=frequency(get(model$series))
resid=model$residuals

#Stationary and Invertible
cat("\nModul of AR Characteristic polynomial Roots: ", 
    Mod(polyroot(c(1,-model$model$phi))),"\n")
cat("\nModul of MA Characteristic polynomial Roots: ",
    Mod(polyroot(c(1,model$model$theta))),"\n")

#Model expressed as an MA infinity (psi-weights)
psis=ARMAtoMA(ar=model$model$phi,ma=model$model$theta,lag.max=36)
names(psis)=paste("psi",1:36)
cat("\nPsi-weights (MA(inf))\n")
cat("\n--------------------\n")
print(psis[1:20])

#Model expressed as an AR infinity (pi-weights)
pis=-ARMAtoMA(ar=-model$model$theta,ma=-model$model$phi,lag.max=36)
names(pis)=paste("pi",1:36)
cat("\nPi-weights (AR(inf))\n")
cat("\n--------------------\n")
print(pis[1:20])
```



```{r echo = FALSE}
model = mod.2

s=frequency(get(model$series))
resid=model$residuals

#Stationary and Invertible
cat("\nModul of AR Characteristic polynomial Roots: ", 
    Mod(polyroot(c(1,-model$model$phi))),"\n")
cat("\nModul of MA Characteristic polynomial Roots: ",
    Mod(polyroot(c(1,model$model$theta))),"\n")

#Model expressed as an MA infinity (psi-weights)
psis=ARMAtoMA(ar=model$model$phi,ma=model$model$theta,lag.max=36)
names(psis)=paste("psi",1:36)
cat("\nPsi-weights (MA(inf))\n")
cat("\n--------------------\n")
print(psis[1:20])

#Model expressed as an AR infinity (pi-weights)
pis=-ARMAtoMA(ar=-model$model$theta,ma=-model$model$phi,lag.max=36)
names(pis)=paste("pi",1:36)
cat("\nPi-weights (AR(inf))\n")
cat("\n--------------------\n")
print(pis[1:20])
```

En el cas del segon model, també es compleix tot i, per tant, també és invertible i estacionari.

### Comparació entre els ACF/PACF mostrals i els ACF/PACF teòrics

Per últim, comparem els valors del ACF i el PACF de les dades amb els valors teòric. S'observa que, en el cas del model $\texttt{mod.1}$, els valors teòrics s'aproximen gairebé perfectament als valors mostrals. En el cas del segon model també es podria dir el mateix, tot i que el quart valor de l'ACF teòric és negatiu i el mostral és positiu. Per tant, ambdós models s'aproximen als valors de ACF/PACF de les mostres, potser una mica millor el $\texttt{mod.1}$.

```{r echo = FALSE}
model = mod.1
dades = d1d12logserie

s=frequency(get(model$series))
resid=model$residuals


#Sample ACF vs. Teoric ACF
par(mfrow=c(2,3),mar=c(3,3,3,3))
acf(dades, ylim=c(-1,1) ,lag.max=36,main="Sample ACF")

plot(ARMAacf(model$model$phi,model$model$theta,lag.max=36),ylim=c(-1,1), 
     type="h",xlab="Lag",  ylab="", main="ACF Teoric mod.1")
abline(h=0)

model = mod.2

plot(ARMAacf(model$model$phi,model$model$theta,lag.max=36),ylim=c(-1,1), 
     type="h",xlab="Lag",  ylab="", main="ACF Teoric mod.1")
abline(h=0)

model = mod.1

#Sample PACF vs. Teoric PACF
pacf(dades, ylim=c(-1,1) ,lag.max=36,main="Sample PACF")

plot(ARMAacf(model$model$phi,model$model$theta,lag.max=36, pacf=T),ylim=c(-1,1),
     type="h", xlab="Lag", ylab="", main="PACF Teoric mod.1")
abline(h=0)

model = mod.2

plot(ARMAacf(model$model$phi,model$model$theta,lag.max=36, pacf=T),ylim=c(-1,1),
     type="h", xlab="Lag", ylab="", main="PACF Teoric mod.1")
abline(h=0)

par(mfrow=c(1,1))
```

## Estabilitat dels Models

Per comprovar l'estabilitat dels models proposats, calculem els models de la serie ocultant les 12 últimes observacions, és a dir, l'últim període d'observacions. Així doncs, s'observa que el valor dels coeficients varia molt poc, de l'ordre de menys de 0.07 en gairebé tots els casos. Per tant, podem confirmar que els models són estables.

```{r echo = FALSE, warning = FALSE}
ultim=c(2017,12)

serie1=window(serie,end=ultim+c(1,0))
lnserie1=log(serie1)
serie2=window(serie,end=ultim)
lnserie2=log(serie2)

cat("########## Model ARIMA(0,1,1)(1,1,0)12 amb i sense les 12 últimes observacions ##########")
cat("\n")

(mod11 <- arima(lnserie1, order=pdq.1, seasonal=list(order=PDQ.1, period=12)))
(mod12 <- arima(lnserie2, order=pdq.1, seasonal=list(order=PDQ.1, period=12)))

cat("\n")
cat("\n")

cat("########## Model ARIMA(1,1,1)(1,1,0)12 amb i sense les 12 últimes observacions ##########")
cat("\n")

(mod21 <- arima(lnserie1, order=pdq.2, seasonal=list(order=PDQ.2, period=12)))
(mod22 <- arima(lnserie2, order=pdq.2, seasonal=list(order=PDQ.2, period=12)))
```

## Capacitat de predicció

A continuació s'avaluarà la capacitat de predicció dels dos models proposats fent-los predir el valor de les 12 útlimes observacions utilitzant la resta d'observacions conegudes.

```{r}
#mod.1

pred=predict(mod12,n.ahead=12)
pr<-ts(c(tail(lnserie2,1),pred$pred),start=ultim,freq=12)

se<-ts(c(0,pred$se),start=ultim,freq=12)

#Intervals
tl<-ts(exp(pr-1.96*se),start=ultim,freq=12)
tu<-ts(exp(pr+1.96*se),start=ultim,freq=12)
pr<-ts(exp(pr),start=ultim,freq=12)


ts.plot(serie,tl,tu,pr,
        lty=c(1,2,2,1),col=c(1,4,4,2),xlim=ultim[1]+c(-3,+2),
        type="o",main="Model mod.1 ARIMA(0,1,1)(1,1,0)12")
abline(v=(ultim[1]-3):(ultim[1]+2),lty=3,col=4)

(previs=window(cbind(tl,pr,tu,serie,error=round(serie-pr,3)),start=ultim))

obs=window(serie,start=ultim)
cat("\n")
cat("###### Errors de predicció del model mod.1 ######\n")
(mod.EQM1=sqrt(sum(((obs-pr)/obs)^2)/12))
(mod.EAM1=sum(abs(obs-pr)/obs)/12)


#mod.2

pred=predict(mod22,n.ahead=12)
pr<-ts(c(tail(lnserie2,1),pred$pred),start=ultim,freq=12)

se<-ts(c(0,pred$se),start=ultim,freq=12)

#Intervals
tl<-ts(exp(pr-1.96*se),start=ultim,freq=12)
tu<-ts(exp(pr+1.96*se),start=ultim,freq=12)
pr<-ts(exp(pr),start=ultim,freq=12)


ts.plot(serie,tl,tu,pr,
        lty=c(1,2,2,1),col=c(1,4,4,2),xlim=ultim[1]+c(-3,+2),
        type="o",main="Model mod.2 ARIMA(1,1,1)(1,1,0)12")
abline(v=(ultim[1]-3):(ultim[1]+2),lty=3,col=4)

(previs=window(cbind(tl,pr,tu,serie,error=round(serie-pr,3)),start=ultim))

obs=window(serie,start=ultim)
cat("\n")
cat("###### Errors de predicció del model mod.2 ######\n")
(mod.EQM1=sqrt(sum(((obs-pr)/obs)^2)/12))
(mod.EAM1=sum(abs(obs-pr)/obs)/12)


```

Com es pot veure, les prediccions (en vermell a la gràfica) de les últimes 12 observacions són semblants i prou bones, ja que en amdós casos s'apropen força a la realitat. A més, el valor real de les observacions (en negre a la gràfica) queda dins l'interval de confiança (en blau a la gràfica) dels valors predits. Per tant, es pot concloure que els models tenen bona capacitat de predicció. A més, els errors de predicció (l'**Error Quadràtic Mitjà** i l'**Error Absolut Mitjà**) dels dos models són semblants i molt petits (> 0.06 en ambdós casos).

## Elecció de model

En definitiva, donat que els dos models han passat la prova de validació i que els dos presenten un comportament similar en la predicció de les últimes 12 observacions, s'escull el primer model, el model $ARIMA(0,1,1)(1,1,0)_{12}$. El principal motiu és que el coeficient que diferencia els dos models surt no significatiu i, per tant, ens quedem amb el model més senzill que, tal i com hem vist, té un bon comportament predictiu i és totalment vàlid.

# Predicció a llarg termini

A continuació, s'utilitza el model escollit $ARIMA(0,1,1)(1,1,0)_{12}$ per predir el valor de la sèrie els 12 mesos posteriors a l'ultima dada que es té. Com es pot observar, el valor predit (en vermell a la gràfica) sembla prou raonable per 2 motius:

- Segueix amb la tendència general de la sèrie de creixement lleu.
- Segueix amb el patró estacional vist al llarg de tota la sèrie: pujada molt pronunciada del número de turistes durant els mesos de primavera i estiu i baixada en picat els mesos de tardor i hivern.

A més, els intervals de confiança també segueixen aquestes tendències estacionals.

```{r echo = FALSE}
##### Previsions a llarg termini amb el model complet ######
mod <- mod.1
pred=predict(mod,n.ahead=12)
pr<-ts(c(tail(logserie,1),pred$pred),start=ultim+c(1,0),freq=12)
se<-ts(c(0,pred$se),start=ultim+c(1,0),freq=12)

#Intervals
tl1<-ts(exp(pr-1.96*se),start=ultim+c(1,0),freq=12)
tu1<-ts(exp(pr+1.96*se),start=ultim+c(1,0),freq=12)
pr1<-ts(exp(pr),start=ultim+c(1,0),freq=12)

ts.plot(serie,tl1,tu1,pr1,
        lty=c(1,2,2,1), col=c(1,4,4,2), xlim=c(ultim[1]-2,ultim[1]+3),
        type="o",main="Model ARIMA(0,1,1)(1,1,0)12")
abline(v=(ultim[1]-2):(ultim[1]+3),lty=3,col=4)
```

```{r echo = FALSE}
(previs1=window(cbind(tl1,pr1,tu1),start=ultim+c(1,0)))
```

# Tractament de *outliers*

Per acabar l'anàlisi d'aquesta serie, es centrarà l'atenció en la detecció i la correcció de possibles *outliers* en la serie. Aquests valors atípics poden ser de tres tipus diferents: 

- **Outlier Aditiu (AO)**: Afecta només a un període ($X_t = 1_{t=TO}(t)$).
- **Canvi Transitori (TC)**: Afecta a un període i el seu efecte decreix exponencialment en els següents períodes ($X_t = \delta^{(t-T0)}1_{t \geq TO}(t)$).
- **Level Shift (LS)**: Afecta a un període i el seu efecte es manté durant els següents períodes ($X_t = 1_{t \geq TO}(t)$).

A continuació es mostren els *outliers* detectats, així com la seva influència sobre les dades i la data en què tenen aquest efecte. S'observa que tenim 10 valors atípics en total, dels quals la majoria són puntuals (6 són AO), 3 són TC i tenim un LS. A més, els que tenen més influència en les dades es concentren tots en l'any 2002, concretament durant els mesos de març, abril i agost.  

```{r echo = FALSE}
source("./airbcn/atipics2.R")

# Detecció outliers
mod.outliers=outdetec(mod,dif=c(1,12),crit=2.8,LS=T)

# Out
outliers=mod.outliers$atip[order(mod.outliers$atip[,1]),]
mesos=c("Gen","Feb","Març","Abr","Maig","Juny","Jul","Ago","Sep","Oct","Nov","Dic")
data.frame(outliers,Fecha=paste(mesos[(outliers[,1]-1)%%12+1],start(logserie)[1]+((outliers[,1]-1)%/%12)))
mod.outliers$sigma2

data.frame(outliers,Fecha=paste(mesos[(outliers[,1]-1)%%12+1],start(logserie)[1]+((outliers[,1]-1)%/%12)),perc.Obs=exp(outliers[,3])*100)
```


```{r echo = FALSE}

##Comparaci?n serie observada con la serie linealizada (sin at?picos)
logserie.lin=lineal(logserie,mod.outliers$atip)
serie.lin=exp(logserie.lin)

plot(serie.lin,col=2)
lines(serie)
```

```{r echo = FALSE}
##Efecto de los at?picos en la serie de logaritmos
plot(logserie-logserie.lin)
```

```{r echo = FALSE}
##Identificaci?n del modelo para la serie linealizada
d1d12lnserie.lin=diff(diff(lnserie.lin,12))
par(mfrow=c(1,2))
acf(d1d12lnserie.lin,ylim=c(-1,1),lag.max=72,col=c(2,rep(1,11)),lwd=2)
pacf(d1d12lnserie.lin,ylim=c(-1,1),lag.max=72,col=c(rep(1,11),2),lwd=2)
par(mfrow=c(1,1))
```

```{r echo = FALSE}
##Estimaci?n del modelo para la serie linealizada
(mod.lin=arima(lnserie.lin,order=pdq.1,seasonal=list(order=PDQ.1,period=12)))
```


```{r echo = FALSE}
dades=d1d12lnserie.lin
model=mod.lin
source("validation.r")
validation(model,dades)
```

```{r echo = FALSE}
########### Estabilitat Modelpara la serie linealizada (SENSE CONSTANT!!!!) ###############
ultim=c(2017,12)

serie1.lin=window(serie.lin,end=ultim+c(1,0))
lnserie1.lin=log(serie1.lin)
serie2.lin=window(serie.lin,end=ultim)
lnserie2.lin=log(serie2.lin)

(mod.lin=arima(lnserie1.lin,order=c(0,1,1),seasonal=list(order=c(2,1,0),period=12)))
(mod2.lin=arima(lnserie2.lin,order=c(0,1,1),seasonal=list(order=c(2,1,0),period=12)))
```


```{r echo = FALSE}
pred=predict(mod2.lin,n.ahead=12)
wLS=sum(mod.atip$atip[mod.atip$atip$type_detected=="LS" & mod.atip$atip$Obs<=length(serie)-12,3])
predic=pred$pr+wLS
pr<-ts(c(tail(lnserie2,1),predic),start=ultim,freq=12)
se<-ts(c(0,pred$se),start=ultim,freq=12)

#Intervals
tl<-ts(exp(pr-1.96*se),start=ultim,freq=12)
tu<-ts(exp(pr+1.96*se),start=ultim,freq=12)
pr<-ts(exp(pr),start=ultim,freq=12)

ts.plot(serie,tl,tu,pr,lty=c(1,2,2,1),col=c(1,4,4,2),xlim=ultim[1]+c(-3,+2),type="o",main="Model ARIMA(0,1,1)(1,1,0)12")
abline(v=(ultim[1]-3):(ultim[1]+2),lty=3,col=4)
```

```{r echo = FALSE}
(previs.lin=window(cbind(tl,pr,tu,serie,error=round(serie-pr,3)),start=ultim))
```

```{r echo = FALSE}
obs=window(serie,start=ultim)
(mod.EQM2=sqrt(sum(((obs-pr)/obs)^2)/12))
(mod.EAM2=sum(abs(obs-pr)/obs)/12)
```


```{r echo = FALSE}
##### Previsions a llarg termini amb el model complet ######

pred=predict(mod.lin,n.ahead=12)
wLS=sum(mod.atip$atip[mod.atip$atip$type_detected=="LS",3])
predic=pred$pr+wLS
pr<-ts(c(lnserie[length(lnserie)],predic),start=ultim+c(1,0),freq=12)
se<-ts(c(0,pred$se),start=ultim+c(1,0),freq=12)

#Intervals
tl2<-ts(exp(pr-1.96*se),start=ultim+c(1,0),freq=12)
tu2<-ts(exp(pr+1.96*se),start=ultim+c(1,0),freq=12)
pr2<-ts(exp(pr),start=ultim+c(1,0),freq=12)

ts.plot(serie,tl2,tu2,pr2,lty=c(1,2,2,1),col=c(1,4,4,2),xlim=ultim[1]+c(-1,+3),type="o",main="Model ARIMA(0,1,1)(2,1,0)12")
abline(v=(ultim[1]-2):(ultim[1]+3),lty=3,col=4)
```

```{r echo = FALSE}
(previs2=window(cbind(tl2,pr2,tu2),start=ultim+c(1,0)))

cbind(previs1,previs2)
```

```{r echo = FALSE}
ts.plot(serie,tl1,tu1,pr1,tl2,tu2,pr2,lty=c(1,2,2,1,2,2,1),col=c(1,4,4,2,3,3,6),xlim=ultim[1]+c(1,3),type="o",main="AIRBCN")
legend("topleft",c("ARIMA(0,1,1)(2,1,0)12","ARIMA(0,1,1)(2,1,0)12 with outlier treatment"),col=c(4,3),lty=1,lwd=2)
abline(v=ultim[1]+1:3,lty=3,col=4)
```

```{r echo = FALSE}
resul=data.frame(
  par=c(length(coef(mod)),length(coef(mod.lin))+nrow(mod.atip$atip)),
  Sigma2Z=c(mod$sigma2,mod.lin$sigma2),
  AIC=c(AIC(mod),AIC(mod.lin)+2*nrow(mod.atip$atip)),
  BIC=c(BIC(mod),BIC(mod.lin)+log(length(serie)-13)*nrow(mod.atip$atip)),
  RMSPE=c(mod.EQM1,mod.EQM2),
  MAPE=c(mod.EAM1,mod.EAM2),
  meanLength=c(sum(previs1[,3]-previs1[,1]),sum(previs2[,3]-previs2[,1]))/12)
row.names(resul)=c("ARIMA(0,1,1)(1,1,0)12","ARIMA(0,1,1)(1,1,0)12+Atip")

resul
```